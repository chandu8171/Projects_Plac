# -*- coding: utf-8 -*-
"""PRML_ASS_3_sms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qAh_YURpsbpTRQfodkxt0ttHVSx7xI7o

## Data extraction and preprocessing
"""

!pip install datasets --quiet

import datasets

dataset = datasets.load_dataset('sms_spam')

dataset

data = dataset['train']

import pandas as pd

data_dict = data.to_dict()

# Convert the dictionary to a pandas DataFrame
df = pd.DataFrame(data_dict)

df

# Splitting the DataFrame into features and target variable
X = df.drop(columns=['label'])  # Replace 'target_column' with the name of your target column
y = df['label']

# Calculating the index to split the DataFrame
split_index = int(len(df) * 0.80)

# Splitting the data into training and testing sets
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1)

train_df

test_df

import re
import string
import pandas as pd
import numpy as np
import nltk
!pip install pyspellchecker
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import string
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

def remove_punctuation(text):
    # Remove punctuation using regex
    return re.sub(r'[^\w\s]', '', text)

def preprocess_text_advanced(text):
    # Remove punctuation
    text = remove_punctuation(text)
    # Convert text to lowercase
    text = text.lower()
    # Handle URLs
    text = re.sub(r'http\S+|www\S+', 'url', text)
    # Handle email addresses
    text = re.sub(r'\S+@\S+', 'email', text)
    # Tokenize text
    tokens = word_tokenize(text)
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    # Join tokens back into a string
    preprocessed_text = ' '.join(lemmatized_tokens)
    return preprocessed_text

# preprocessing steps to the "sms" column of the train_df DataFrame
train_df['clean_sms'] = train_df['sms'].apply(preprocess_text_advanced)

# preprocessing steps to the "sms" column of the test_df DataFrame
test_df['clean_sms'] = test_df['sms'].apply(preprocess_text_advanced)

train_df

test_df

ham_messages = train_df[train_df['label'] == 0]['clean_sms']
spam_messages = train_df[train_df['label'] == 1]['clean_sms']

# Calculate the total number of messages
total_messages = len(train_df)

# Calculate the probability of spam
probability_of_spam = len(spam_messages) / total_messages

# Print the probability of spam
print(f"Probability of spam: {probability_of_spam:.2%}")

def count_vectorizer(corpus, binary=True):
    # Create a vocabulary from the corpus
    vocabulary = set()
    for message in corpus:
        vocabulary.update(message.split())

    # Map each word to an index in the vocabulary
    word_to_index = {word: index for index, word in enumerate(sorted(vocabulary))}

    # Initialize an empty matrix to store the binary vector representations
    binary_vectors = []
    for message in corpus:
        # Initialize a binary vector for the current message
        binary_vector = [0] * len(vocabulary)
        # Update the binary vector based on the presence of words in the message
        for word in message.split():
            if word in word_to_index:
                binary_vector[word_to_index[word]] = 1 if binary else 1  # If binary is False, use count instead of binary
        binary_vectors.append(binary_vector)

    # Create a DataFrame from the binary vectors
    df = pd.DataFrame(binary_vectors, columns=sorted(vocabulary))
    df['label'] = np.nan  # Initialize label column with NaN values
    return df

# Combine messages from train and test datasets
combined_messages = pd.concat([train_df['clean_sms'], test_df['clean_sms']])

# Create the count vectorizer using the combined messages
combined_vectorizer = count_vectorizer(combined_messages)

# Get labels from train and test datasets
combined_labels = pd.concat([train_df['label'], test_df['label']])

# Assign labels to the combined vectorized messages
combined_vectorizer['label'] = combined_labels

# Separate the vectorized messages back into train and test datasets
train_df_vectorized = combined_vectorizer[:len(train_df)]
test_df_vectorized = combined_vectorizer[len(train_df):]

train_df_vectorized

test_df_vectorized

"""## Naive Bayes"""

total_messages = len(train_df_vectorized)
num_spam_messages = train_df_vectorized['label'].sum()
num_ham_messages = total_messages - num_spam_messages
p_spam = num_spam_messages / total_messages
p_ham = num_ham_messages / total_messages

num_spam_messages

p_ham

spam_messages_vectorized = train_df_vectorized[train_df_vectorized['label'] == 1].drop(columns=['label'])
ham_messages_vectorized = train_df_vectorized[train_df_vectorized['label'] == 0].drop(columns=['label'])

# Calculate the conditional probabilities p(xi | y) for spam and ham classes
p_word_given_spam = (spam_messages_vectorized.sum() + 1) / (num_spam_messages + 2)  # Laplace smoothing
p_word_given_ham = (ham_messages_vectorized.sum() + 1) / (num_ham_messages + 2)  # Laplace smoothing

spam_messages_vectorized.sum()

p_word_given_spam

print(p_word_given_ham.shape, train_df_vectorized.shape)

def predict_class_for_message(message_vector, p_spam, p_ham, p_word_given_spam, p_word_given_ham):
    # Calculate the probability of the message being spam
    spam_probability = np.prod((p_word_given_spam ** message_vector) * ((1 - p_word_given_spam) ** (1 - message_vector))) * p_spam
    # Calculate the probability of the message being ham
    ham_probability = np.prod((p_word_given_ham ** message_vector) * ((1 - p_word_given_ham) ** (1 - message_vector))) * p_ham
    if spam_probability > ham_probability:
        return 1  # Spam
    else:
        return 0  # Ham

predicted_labels = train_df_vectorized.apply(lambda row: predict_class_for_message(row.drop('label'), p_spam, p_ham, p_word_given_spam, p_word_given_ham), axis=1)

# Calculate accuracy
accuracy = (predicted_labels == train_df_vectorized['label']).mean()
print("Accuracy on train_df_vectorized:", accuracy)

predicted_labels_test = test_df_vectorized.apply(lambda row: predict_class_for_message(row.drop('label'), p_spam, p_ham, p_word_given_spam, p_word_given_ham), axis=1)

# Calculate accuracy
accuracy = (predicted_labels_test == test_df_vectorized['label']).mean()
print("Accuracy on test_df_vectorized:", accuracy)

from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix
confusion_matrix = confusion_matrix(train_df_vectorized['label'], predicted_labels)

# Print the confusion matrix
print(confusion_matrix)

"""## SVM"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Initialize SVM classifier
svm_classifier = SVC(kernel='linear')

# Train the SVM classifier on the vectorized training data
svm_classifier.fit(train_df_vectorized.drop('label', axis=1), train_df_vectorized['label'])

# Predict the labels for the test_df_vectorized data
predicted_labels_test_svm = svm_classifier.predict(test_df_vectorized.drop('label', axis=1))

# Calculate the accuracy of the SVM classifier on the test_df_vectorized data
accuracy_svm = accuracy_score(test_df_vectorized['label'], predicted_labels_test_svm)
print("Accuracy of SVM classifier on test data:", accuracy_svm)

"""## DT"""

import numpy as np

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

class DecisionTree:
    def __init__(self, max_depth=None, max_features=None):
        self.max_depth = max_depth
        self.max_features = max_features
        self.root = None

    def fit(self, X, y):
        self.root = self._grow_tree(X, y, depth=0)

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))

        if (self.max_depth is not None and depth >= self.max_depth) or n_labels == 1 or n_samples < 2:
            return Node(value=self._most_common_label(y))

        # Random feature selection
        if self.max_features is not None:
            features = np.random.choice(n_features, self.max_features, replace=False)
        else:
            features = range(n_features)

        # Find the best split among selected features
        best_gain = 0
        best_feature = None
        best_threshold = None
        for feature in features:
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                gain = self._information_gain(X, y, feature, threshold)
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold

        if best_gain == 0:
            return Node(value=self._most_common_label(y))
        left_indices = X[:, best_feature] <= best_threshold
        right_indices = ~left_indices
        left_child = self._grow_tree(X[left_indices], y[left_indices], depth + 1)
        right_child = self._grow_tree(X[right_indices], y[right_indices], depth + 1)

        return Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)

    def _most_common_label(self, y):
        return np.bincount(y).argmax()

    def _information_gain(self, X, y, feature, threshold):
        parent_entropy = self._entropy(y)
        left_indices = X[:, feature] <= threshold
        right_indices = ~left_indices
        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:
            return 0
        left_entropy = self._entropy(y[left_indices])
        right_entropy = self._entropy(y[right_indices])
        child_entropy = (np.sum(left_indices) / len(y)) * left_entropy + (np.sum(right_indices) / len(y)) * right_entropy
        return parent_entropy - child_entropy

    def _entropy(self, y):
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))

    def predict(self, X):
        return np.array([self._predict_single(sample, self.root) for sample in X])

    def _predict_single(self, sample, node):
        if node.value is not None:
            return node.value
        if sample[node.feature] <= node.threshold:
            return self._predict_single(sample, node.left)
        else:
            return self._predict_single(sample, node.right)

# Initialize and train Decision Tree classifier
dt_classifier_scratch = DecisionTree(max_depth=36, max_features=None)
X_train_scratch = train_df_vectorized.drop('label', axis=1).values
y_train_scratch = train_df_vectorized['label'].values
dt_classifier_scratch.fit(X_train_scratch, y_train_scratch)

# Predict the labels for the test data
X_test_scratch = test_df_vectorized.drop('label', axis=1).values
predicted_labels_test_dt_scratch = dt_classifier_scratch.predict(X_test_scratch)

# Calculate the accuracy of the Decision Tree classifier
accuracy_dt_scratch = accuracy_score(test_df_vectorized['label'], predicted_labels_test_dt_scratch)
print("Accuracy of Decision Tree classifier (from scratch) on test data:", accuracy_dt_scratch)

"""## Logistic Regression"""

import numpy as np
class LogisticRegression:
    def __init__(self, learning_rate=0.01, max_iter=100):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.max_iter):
            z = np.dot(X, self.weights) + self.bias
            y_pred = 1 / (1 + np.exp(-z))
            gradient_weights = -np.dot(X.T, (y - y_pred))
            gradient_bias = -np.sum(y - y_pred)

            self.weights -= self.learning_rate * gradient_weights
            self.bias -= self.learning_rate * gradient_bias

    def predict(self, X):
        z = np.dot(X, self.weights) + self.bias
        y_pred = 1 / (1 + np.exp(-z))
        return np.where(y_pred > 0.5, 1, 0)

# Initialize and train Logistic Regression classifier
lr_classifier = LogisticRegression(learning_rate=0.015, max_iter=450)
X_train = train_df_vectorized.drop('label', axis=1).values
y_train = train_df_vectorized['label'].values
lr_classifier.fit(X_train, y_train)

# Predict the labels for the test data
X_test = test_df_vectorized.drop('label', axis=1).values
predicted_labels_test_lr = lr_classifier.predict(X_test)

# Calculate the accuracy of the Logistic Regression classifier
accuracy_lr = accuracy_score(test_df_vectorized['label'], predicted_labels_test_lr)
print("Accuracy of Logistic Regression classifier (from scratch) on test data:", accuracy_lr)

"""### final results updated set"""

import os

test_directory = "test"
if not os.path.exists(test_directory):
    os.makedirs(test_directory)

for index, row in test_df.iterrows():
    email_text = row['sms']
    email_filename = f"email{index + 1}.txt"
    with open(os.path.join(test_directory, email_filename), "w") as file:
        file.write(email_text)

print("Test emails have been saved to the 'test' directory.")

folder_path = "test"
for i, filename in enumerate(os.listdir(folder_path)):
    if filename.startswith("email") and filename.endswith(".txt"):
        # Update label in the email folder based on predicted label
        new_label = predicted_labels_test_lr[i]
        # Your code to update labels in the email folder goes here
        print(f"Updated label for {filename}: {new_label}")